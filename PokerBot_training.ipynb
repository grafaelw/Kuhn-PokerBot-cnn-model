{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PokerBot-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLqEouDIu39xD63EJ7D5dG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grafaelw/Kuhn-PokerBot-cnn-model/blob/main/PokerBot_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGtYr7KweZRT"
      },
      "outputs": [],
      "source": [
        "! rm -rf data_sets/\n",
        "! pip install -r requirements-linux.txt\n",
        "! python data_sets.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the training data\n"
      ],
      "metadata": {
        "id": "9MJIAmXJg1pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import files\n",
        "\n",
        "img_rows, img_cols = 32, 32\n",
        "batch_size = 32\n",
        "ROTATE_MAX_ANGLE = 15\n",
        "\n",
        "dir_path = os.getcwd()\n",
        "train_data_dir = dir_path + '/data_sets/training_images'\n",
        "\n",
        "LABELS = ['J','Q','K','A']\n",
        "\n",
        "features_gen = ImageDataGenerator(rescale=1. / 255,\n",
        "                                  shear_range=0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                  rotation_range=2*ROTATE_MAX_ANGLE,\n",
        "                                  horizontal_flip=True,\n",
        "                                  validation_split=0.2,\n",
        "                                  fill_mode='nearest')\n",
        "\n",
        "\n",
        "train_gen = features_gen.flow_from_directory(train_data_dir, color_mode='grayscale',\n",
        "                                            target_size=(img_rows, img_cols),\n",
        "                                            batch_size=batch_size,\n",
        "                                            class_mode='categorical',\n",
        "                                            subset='training', shuffle=True)\n",
        "\n",
        "val_gen = features_gen.flow_from_directory(train_data_dir, color_mode='grayscale',\n",
        "                                          target_size=(img_rows, img_cols),\n",
        "                                          batch_size=batch_size, \n",
        "                                          class_mode='categorical',\n",
        "                                          subset='validation', shuffle=True)\n",
        "                                          \n",
        "train_ds = tf.data.Dataset.from_generator(lambda: train_gen, output_types=(tf.float32,tf.float32), \n",
        "                                          output_shapes=([None, img_rows, img_cols, 1],[None, len(LABELS)]))\n",
        "val_ds = tf.data.Dataset.from_generator(lambda: val_gen, output_types=(tf.float32,tf.float32), \n",
        "                                        output_shapes=([None, img_rows, img_cols, 1],[None, len(LABELS)]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IERduK44g1YP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc9c579-5300-4741-87b1-da4b68d26a6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16002 images belonging to 4 classes.\n",
            "Found 3998 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model"
      ],
      "metadata": {
        "id": "jur9p8jz5cIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "    \n",
        "# Feature learning 0\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(img_rows, img_cols, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(img_rows, img_cols, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.1))\n",
        "    \n",
        "# Feature learning 1\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# Fully-connected layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu',kernel_initializer='he_uniform'))\n",
        "model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(len(LABELS), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy',\"mean_squared_error\"])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "bosG51THevZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fc23d9-d349-445a-dc54-a739ae9407fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 10, 10, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 10, 10, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               819456    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 1,213,636\n",
            "Trainable params: 1,212,868\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "nCTp5qP3g3fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, # Stopping the training if desired validation loss is reached or don't change after 3 epochs\n",
        "                           verbose=1, restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, # Reducing the learning rate if desired validation loss is reached or don't change after 3 epochs\n",
        "                              verbose=1, min_delta=0.0001, min_lr=0, cooldown=0) \n",
        "\n",
        "callbacks = [early_stop, reduce_lr]\n",
        "\n",
        "model.fit(train_gen, epochs=10, callbacks=callbacks, validation_data=val_gen)\n",
        "model.save('/model/model.h5', overwrite=True) # Saving the trained model"
      ],
      "metadata": {
        "id": "_mLFsiVGg312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4cd3ac-37ce-4b95-eed5-77e2a991aa2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "501/501 [==============================] - 210s 418ms/step - loss: 0.0271 - accuracy: 0.9968 - mean_squared_error: 0.0015 - val_loss: 0.0436 - val_accuracy: 0.9967 - val_mean_squared_error: 0.0015\n",
            "Epoch 2/20\n",
            "501/501 [==============================] - 201s 400ms/step - loss: 0.0117 - accuracy: 0.9986 - mean_squared_error: 6.2783e-04 - val_loss: 1.7317e-04 - val_accuracy: 1.0000 - val_mean_squared_error: 2.3190e-05\n",
            "Epoch 3/20\n",
            "501/501 [==============================] - 219s 438ms/step - loss: 5.1420e-04 - accuracy: 0.9998 - mean_squared_error: 8.1531e-05 - val_loss: 0.0045 - val_accuracy: 0.9997 - val_mean_squared_error: 1.2506e-04\n",
            "Epoch 4/20\n",
            "501/501 [==============================] - 217s 433ms/step - loss: 0.0073 - accuracy: 0.9993 - mean_squared_error: 3.7004e-04 - val_loss: 0.0033 - val_accuracy: 0.9995 - val_mean_squared_error: 1.8946e-04\n",
            "Epoch 5/20\n",
            "501/501 [==============================] - 192s 383ms/step - loss: 0.0066 - accuracy: 0.9990 - mean_squared_error: 4.2341e-04 - val_loss: 0.0142 - val_accuracy: 0.9985 - val_mean_squared_error: 6.8607e-04\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 00005: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the model with test dataset\n",
        "\n",
        "At this stage, we have trained our model and it is the time that we should evaluate our trained model whether it is good enough for prediction or not. \n",
        "For now, the model is based on a strong notions of VGG-5 neural network architecture which known for its high accuracy and low MSE (Mean Squared Error) value."
      ],
      "metadata": {
        "id": "ATYRg__vuh6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_dir = dir_path + '/data_sets/test_images'\n",
        "\n",
        "\n",
        "testing_gen = ImageDataGenerator(rescale=1. / 255,shear_range=0.2,\n",
        "                                 zoom_range=0.2, horizontal_flip=True,\n",
        "                                 rotation_range=2*ROTATE_MAX_ANGLE,\n",
        "                                 fill_mode='nearest')\n",
        "\n",
        "\n",
        "test_gen = testing_gen.flow_from_directory(test_data_dir, color_mode='grayscale',\n",
        "                                            target_size=(img_rows, img_cols),\n",
        "                                            batch_size=batch_size,\n",
        "                                            class_mode='categorical',\n",
        "                                            subset='training', shuffle=True)\n",
        "\n",
        "\n",
        "                                          \n",
        "test_ds = tf.data.Dataset.from_generator(lambda: test_gen, output_types=(tf.float32,tf.float32), \n",
        "                                          output_shapes=([None, img_rows, img_cols, 1],[None, len(LABELS)]))\n",
        "\n",
        "\n",
        "score = model.evaluate(test_gen, verbose=1)\n",
        "\n",
        "print(f\"Test Loss = {score[0]*100}%\")\n",
        "print(f\"Test Accuracy = {score[1]*100}%\")"
      ],
      "metadata": {
        "id": "PDLU3qdGug-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4d3b43-716a-4777-8b4c-1c8ed09d27b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 images belonging to 4 classes.\n",
            "125/125 [==============================] - 17s 136ms/step - loss: 1.1436e-05 - accuracy: 1.0000 - mean_squared_error: 2.3532e-07\n",
            "Test Loss = 0.0011435676242399495%\n",
            "Test Accuracy = 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uyL43TqkugwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(image: Image):\n",
        "    features = np.array(image)\n",
        "    features = features.reshape(1,features.shape[0], features.shape[1], 1).astype('float32') / 255\n",
        "    return features\n",
        "\n",
        "\n",
        "labels = {0:'A', 1:'J', 2:'K', 3:'Q'}\n",
        "test = [extract_features(Image.open(test_data_dir + '/A/0.png')), # Note: change the file number, because it is randomly generated\n",
        "        extract_features(Image.open(test_data_dir + '/J/1.png')), # Note: change the file number, because it is randomly generated\n",
        "        extract_features(Image.open(test_data_dir + '/Q/10.png')), # Note: change the file number, because it is randomly generated\n",
        "        extract_features(Image.open(test_data_dir + '/K/100.png'))] # Note: change the file number, because it is randomly generated\n",
        "\n",
        "rank = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  rank.append(labels[np.argmax(model.predict(test[i]))])\n",
        "  print(f\"Prediction {i+1} = {rank[i]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79nxK6bBj3Ym",
        "outputId": "5297dc12-a5ae-473d-93a8-63c84907b6df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction 1 = A\n",
            "Prediction 2 = J\n",
            "Prediction 3 = Q\n",
            "Prediction 4 = K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nym95l7wj23p"
      }
    }
  ]
}